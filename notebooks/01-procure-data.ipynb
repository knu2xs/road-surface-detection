{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Prepare Training Data\n",
    "\n",
    "Typically the most difficult and time consuming task for any supervised deep learning project is creating quality training data. Thankfully, in this case, there is a dataset available we can use. Still, we have to get and organize this trainig data before we can get started. This notebook takes care of these tasks, and _only_ needs to be run once.\n",
    "\n",
    "__NOTE__: Each of these cells can be run _even if run before_, since they check if the results are already there before doing a long running process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import arcpy\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Project Data Directories\n",
    "\n",
    "The project structure keeps data organized in subdirectories within a data directory. If you just cloned this repo, these directories do not exist, so first we are going to ensure this directory structure is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to common data locations - NOTE: to convert any path to a raw string, simply use str(path_instance)\n",
    "dir_prj = Path.cwd().parent\n",
    "\n",
    "dir_data = dir_prj/'data'\n",
    "\n",
    "dir_raw = dir_data/'raw'\n",
    "dir_ext = dir_data/'external'\n",
    "dir_int = dir_data/'interim'\n",
    "dir_out = dir_data/'processed'\n",
    "\n",
    "dir_models = dir_prj/'models'\n",
    "\n",
    "gdb_int = dir_int/'interim.gdb'\n",
    "gdb_out = dir_out/'processed.gdb'\n",
    "\n",
    "# make sure data directories exist\n",
    "for dir_dta in (dir_raw, dir_ext, dir_int, dir_out, dir_models):\n",
    "    if not dir_dta.exists():\n",
    "        dir_dta.mkdir(parents=True)\n",
    "        \n",
    "# make sure the geodatabases exist\n",
    "for gdb in [gdb_int, gdb_out]:\n",
    "    if not arcpy.Exists(str(gdb)):\n",
    "        arcpy.management.CreateFileGDB(str(gdb.parent), str(gdb.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "The data used for training is freely available for download. Hence, the first step is procuring the archive by downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading D:\\projects\\road-surface-detection\\data\\raw\\RoadDamageDataset.tar.gz from https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/RoadDamageDataset.tar.gz.\n",
      "\n",
      "This is going to take a while. Go for a walk. Watch a sitcom. Do something else for a while.\n",
      "\n",
      "Download finally complete. At least you only have to do that once, right?\n",
      "\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "archive_url = 'https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/RoadDamageDataset.tar.gz'\n",
    "archive_pth = dir_raw/archive_url.split('/')[-1]\n",
    "\n",
    "if not archive_pth.exists():\n",
    "    \n",
    "    print(f'Downloading {archive_pth} from {archive_url}.\\n\\nThis is going to take a while. Go for a walk. Watch a sitcom. Do something else for a while.')\n",
    "    \n",
    "    with requests.get(archive_url, stream=True) as req:\n",
    "        \n",
    "        req.raise_for_status()\n",
    "        \n",
    "        with open(archive_pth, 'wb') as out_file:\n",
    "            \n",
    "            for chunk in req.iter_content(chunk_size=8192):\n",
    "                \n",
    "                out_file.write(chunk)\n",
    "    \n",
    "    print('\\nDownload finally complete. At least you only have to do that once, right?\\n')\n",
    "                \n",
    "else:\n",
    "    \n",
    "    print(f'{archive_pth} has already been downloaded. Lucky you.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack the Archive\n",
    "\n",
    "Next, although we have the data, it is still all wrapped up in an archive, so we are going to have to unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to extract D:\\projects\\road-surface-detection\\data\\raw\\RoadDamageDataset.tar.gz to D:\\projects\\road-surface-detection\\data\\raw\\RoadDamageDataset.\n",
      "\n",
      "Finally finsihed extracting.\n",
      "\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dir_raw_training = dir_raw/'RoadDamageDataset'\n",
    "\n",
    "if not dir_raw_training.exists():\n",
    "    \n",
    "    print(f'Starting to extract {archive_pth} to {dir_raw_training}.\\n')\n",
    "    \n",
    "    with tarfile.open(archive_pth, 'r:gz') as gz_fl:\n",
    "        gz_fl.extractall(dir_raw)\n",
    "    \n",
    "    print('Finally finsihed extracting.\\n')\n",
    "\n",
    "else:\n",
    "    print('Training data already extracted - lucky you.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize Data\n",
    "\n",
    "Even with the data extracted, we still need the data organized so the dataloader will where to find the images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dir_training = dir_raw/'training_data'\n",
    "\n",
    "assert dir_raw_training.exists()\n",
    "\n",
    "dir_images = dir_training/'images'\n",
    "dir_labels = dir_training/'labels'\n",
    "\n",
    "dir_images.mkdir(parents=True, exist_ok=True)\n",
    "dir_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not(len(list(dir_images.glob('*'))) and len(list(dir_labels.glob('*')))):\n",
    "\n",
    "    for fl in os.listdir(dir_raw_training):\n",
    "        if not(fl.startswith(\".\")):\n",
    "            for f in os.listdir(dir_raw_training/fl/'Annotations'):\n",
    "                if not(f.startswith(\".\")):\n",
    "                    img_name = f.split('.')[0] + '.jpg'\n",
    "\n",
    "                    shutil.copyfile(dir_raw_training/fl/'JPEGImages'/img_name, dir_training/'images'/img_name)\n",
    "                    shutil.copyfile(dir_raw_training/fl/'Annotations'/f, dir_training/'labels'/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Standard",
   "notebookRuntimeVersion": "4.0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
